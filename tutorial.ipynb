{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark / PySpark tutorial\n",
    "========================\n",
    "\n",
    "Spark is an increasingly popular cluster computing system based on Apache Hadoop that offers great potential value because of its speed and ease of use. We are going to have a look at it here, with special focus on the Python interface to Spark: PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisites\n",
    "-------------\n",
    "\n",
    "Download one of the pre-built versions of Spark from the website (http://spark.apache.org/downloads.html), and untar it (`tar -xvf <filename>`). Alternatively, build Spark yourself by running `./sbt/sbt assembly` from the Spark directory.\n",
    "\n",
    "We will run Spark locally on our machine in this tutorial. After you set the `SPARK_HOME` environment variable and add Spark's `python` directory to the `PYTHONPATH`, you're good to go.\n",
    "\n",
    "*Note:\n",
    "When running locally and running inside a virtualenv we need to tell Spark to use the current version of Python, otherwise it would use the default system Python version. Put this in your code: `os.environ['PYSPARK_PYTHON'] = sys.executable`.*\n",
    "\n",
    "*Note:\n",
    "Spark has a web UI that shows running tasks, workers and various statistics. Running locally, it can be reached at http://localhost:4040/.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling PySpark\n",
    "---------------\n",
    "\n",
    "To call Spark from Python, we need to use the PySpark interface. It can, for example, be called as an interactive shell from your Spark home dir:\n",
    "\n",
    "    ./bin/pyspark\n",
    "\n",
    "As an iPython Spark shell:\n",
    "\n",
    "    IPYTHON=1 ./bin/pyspark\n",
    "\n",
    "Or as a launcher for scripts:\n",
    "\n",
    "    ./bin/pyspark --master local\n",
    "\n",
    "Below we show how you would use the PySpark API inside a Python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-123771c84655>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# OK, now we can import PySpark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Spark's home directory (here it's: ~/spark-1.6.0) should be set as an environment variable.\n",
    "# (Of course setting an env. variable doesn't need to be done from Python; any method will do.)\n",
    "os.environ['SPARK_HOME'] = os.path.join(os.path.expanduser('~'), 'spark-1.6.0')\n",
    "\n",
    "# Add Spark's Python interface (PySpark) to PYTHONPATH.\n",
    "# (Again: this doesn't need to be done from Python.)\n",
    "sys.path.append(os.path.join(os.environ.get('SPARK_HOME'), 'python'))\n",
    "\n",
    "# This can be useful for running in virtualenvs:\n",
    "# os.environ['PYSPARK_PYTHON'] = '/home/user/virtualenv/bin/python'\n",
    "\n",
    "# OK, now we can import PySpark\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside our *driver program*, the connection to Spark is represented by a `SparkContext` instance. For running Spark locally, you can simply instantiate one with:\n",
    "\n",
    "    sc = SparkContext('local', 'mySparkApp')\n",
    "\n",
    "Alternatively, you can use a `SparkConf` instance to control various Spark configuration properties, which is what we are going to demonstrate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkConf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6d3d57600417>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# To run local, use:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'local'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spark_tutorial'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# SparkConf's 'set', 'setAll' and 'setIfMissing' can be used to set a variety of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SparkConf' is not defined"
     ]
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "# To run local, use:\n",
    "conf.setMaster('local')\n",
    "conf.setAppName('spark_tutorial')\n",
    "# SparkConf's 'set', 'setAll' and 'setIfMissing' can be used to set a variety of\n",
    "# configuration properties, e.g.:\n",
    "conf.setIfMissing(\"spark.cores.max\", \"4\")\n",
    "conf.set(\"spark.executor.memory\", \"1g\")\n",
    "# Alternatively:\n",
    "conf.setAll([('spark.cores.max', '4'), ((\"spark.executor.memory\", \"1g\"))])\n",
    "\n",
    "# Now instantiate SparkContext\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# Note: SparkContexts can be stopped manually with:\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Spark works, in brief\n",
    "-------------------------\n",
    "\n",
    "Spark uses a *cluster manager* (e.g., Spark's own standalone manager, YARN or Mesos), and a number of *worker nodes*. The manager attempts to acquire *executors* on the worker nodes, which do computations and store data based on the code and tasks that are sent to them.\n",
    "\n",
    "Spark's primary abstraction is a so-called *Resilient Distributed Dataset (RDD)*. Spark can create RDDs from any storage source supported by Hadoop. An RDD holds intermediate computational results and is stored in RAM or on disk across the worker nodes. In case a node fails, an RDD can be restored. Many processes can executed in parallel thanks to the distributed nature of RDDs, and pipelining and lazy execution prevent the need for saving intermediate results for the next step. Importantly, Spark supports pulling data sets into a cluster-wide *in-memory cache* for fast access.\n",
    "\n",
    "RDD operations can be divided into 2 groups: *transformations* and *actions*. Transformations (e.g., `map`) of RDDs always result in new RDDs, and actions (e.g., `reduce`) return values that are the result of operations on the RDD back to the driver program.\n",
    "\n",
    "The above and more will be demonstrated in the code examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs are distributed data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 4\n",
      "[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11, 12, 13]]\n"
     ]
    }
   ],
   "source": [
    "# 'parallelize' creates an RDD by distributing data over the cluster\n",
    "rdd = sc.parallelize(range(14))\n",
    "print(\"Number of partitions: {}\".format(rdd.getNumPartitions()))\n",
    "# 'glom' lists all elements within each partition\n",
    "print(rdd.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark is lazy\n",
    "Despite any intermediate transformations, Spark only runs after an *action* is performed on an RDD. This is because it tries to do smart pipelining of operations so that it does not have to save intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169]\n"
     ]
    }
   ],
   "source": [
    "rddSquared = rdd.map(lambda x: x ** 2)\n",
    "# Alternatively, you can use a normal function:\n",
    "# def squared(x):\n",
    "#     return x ** 2\n",
    "# rddSquared = rdd.map(squared)\n",
    "\n",
    "# The 'collect' action triggers Spark: the above transformation is performed,\n",
    "# and results are collected.\n",
    "print(rddSquared.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "0\n",
      "[0, 1, 2, 3, 4]\n",
      "[13, 12, 11]\n",
      "[13, 12, 11, 10, 9, 8, 7]\n"
     ]
    }
   ],
   "source": [
    "# Popular transformations\n",
    "# -----------------------\n",
    "\n",
    "func = lambda x: -x\n",
    "rdd.map(func)\n",
    "rdd.flatMap(func) # like map, but flattens results\n",
    "rdd.filter(func)\n",
    "rdd.sortBy(func)\n",
    "\n",
    "# Popular actions\n",
    "# ---------------\n",
    "\n",
    "rdd.reduce(lambda x, y: x + y)\n",
    "rdd.count()\n",
    "\n",
    "# Actions with which to take data from an RDD:\n",
    "print(rdd.collect())                    # get all elements\n",
    "print(rdd.first())                      # get first element\n",
    "print(rdd.take(5))                      # get N first elements\n",
    "print(rdd.top(3))                       # get N highest elements in descending order\n",
    "print(rdd.takeOrdered(7, lambda x: -x)) # get N first elements in ascending (or a function's) order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs can be cached to memory (or drive)\n",
    "Spark allows the user to control what data is cached and how. Proper caching of RDDs can be hugely beneficial! Whenever you have an RDD that will be re-used multiple times later on, you should consider caching it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized 1x Replicated\n",
      "1 loops, best of 1: 2.4 s per loop\n",
      "1 loops, best of 1: 1.41 s per loop\n",
      "1 loops, best of 1: 1.62 s per loop\n",
      "1 loops, best of 1: 1.33 s per loop\n",
      "Memory Serialized 1x Replicated\n",
      "1 loops, best of 1: 2.66 s per loop\n",
      "1 loops, best of 1: 916 ms per loop\n",
      "1 loops, best of 1: 786 ms per loop\n",
      "1 loops, best of 1: 1.16 s per loop\n",
      "Serialized 1x Replicated\n",
      "Disk Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "NUM_SAMPLES = int(1e6)\n",
    "rddBig = sc.parallelize(np.random.random(NUM_SAMPLES))\n",
    "\n",
    "# no caching: will be recalculated everytime we go through the loop\n",
    "rddBigTrans = rddBig.map(lambda x: (x ** 2 - 0.1) ** 0.5)\n",
    "print(rddBigTrans.getStorageLevel())\n",
    "for threshold in (0.2, 0.4, 0.6, 0.8):\n",
    "    %timeit -n 1 -r 1 rddBigTrans.filter(lambda x: x >= threshold).count()\n",
    "\n",
    "# we cache this intermediate result because it will be repeatedly called\n",
    "rddBigTrans_c = rddBig.map(lambda x: (x ** 2 - 0.1) ** 0.5).cache()\n",
    "print(rddBigTrans_c.getStorageLevel())\n",
    "for threshold in (0.2, 0.4, 0.6, 0.8):\n",
    "    %timeit -n 1 -r 1 rddBigTrans_c.filter(lambda x: x >= threshold).count()\n",
    "\n",
    "# use unpersist to remove from cache\n",
    "print(rddBigTrans_c.unpersist().getStorageLevel())\n",
    "# for even finer-grained control of caching, use the 'persist' function\n",
    "from pyspark import storagelevel\n",
    "print(rddBigTrans.persist(storagelevel.StorageLevel.MEMORY_AND_DISK_SER).getStorageLevel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark can work with key-value pairs\n",
    "So-called PairRDDs are RDDs that store key-value pairs. Spark has a variety of special operations that make use of this, such as joining by key, grouping by key, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipped: [(0, 0), (1, 1), (2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 8), (9, 9), (10, 10), (11, 11), (12, 12), (13, 13)]\n",
      "Keys: [True, False, True, False, True, False, True, False, True, False, True, False, True, False]\n",
      "Values: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "[(True, 0), (False, 1), (True, 4), (False, 9), (True, 16), (False, 25), (True, 36), (False, 49), (True, 64), (False, 81), (True, 100), (False, 121), (True, 144), (False, 169)]\n",
      "[(True, 0), (False, 1), (True, 4), (False, 9), (True, 16), (False, 25), (True, 36), (False, 49), (True, 64), (False, 81), (True, 100), (False, 121), (True, 144), (False, 169)]\n",
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169]\n"
     ]
    }
   ],
   "source": [
    "# PairRDDs are automatically created whenever we present a list of key-value tuples\n",
    "# Here we transform rddA and create a key based on even/odd flags\n",
    "rddP1 = rdd.map(lambda x: (x % 2 == 0, x))\n",
    "# A clearer shortcut for this is:\n",
    "rddP1 = rdd.keyBy(lambda x: x % 2 == 0)\n",
    "\n",
    "# Another way to create a PairRDD is to zip two RDDs (assumes equal length RDDs)\n",
    "print(\"Zipped: {}\".format(rdd.zip(rdd).collect()))\n",
    "\n",
    "# Access to the keys and values\n",
    "print(\"Keys: {}\".format(rddP1.keys().collect()))\n",
    "print(\"Values: {}\".format(rddP1.values().collect()))\n",
    "\n",
    "# This is how you can map a function to a pairRDD; x[0] is the key, x[1] the value\n",
    "print(rddP1.map(lambda x: (x[0], x[1] ** 2)).collect())\n",
    "# Better: mapValues/flatMapValues, which operates on values only and keeps the keys in place\n",
    "print(rddP1.mapValues(lambda x: x ** 2).collect())\n",
    "# We can also go back from a PairRDD to a normal RDD by simply dropping the key\n",
    "print(rddP1.map(lambda x: x[1] ** 2).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum per key: [(False, 49), (True, 42)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(<type 'int'>, {False: 7, True: 7})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Various aggregations by key are possible, such as reduceByKey, combineByKey, and foldByKey\n",
    "# reduceByKey example:\n",
    "print(\"Sum per key: {}\".format(rddP1.reduceByKey(lambda x, y: x + y).collect()))\n",
    "\n",
    "# Also, some common operation are available in 'ByKey' form, e.g.:\n",
    "rddP1.sortByKey()\n",
    "rddP1.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join: [(True, (6, 0)), (True, (6, 2)), (True, (6, 4)), (True, (6, 6)), (True, (6, 8)), (True, (6, 10)), (True, (6, 18)), (True, (6, 20)), (True, (6, 22)), (True, (6, 24)), (True, (6, 26)), (True, (6, 12)), (True, (6, 14)), (True, (6, 16)), (True, (8, 0)), (True, (8, 2)), (True, (8, 4)), (True, (8, 6)), (True, (8, 8)), (True, (8, 10)), (True, (8, 18)), (True, (8, 20)), (True, (8, 22)), (True, (8, 24)), (True, (8, 26)), (True, (8, 12)), (True, (8, 14)), (True, (8, 16)), (True, (4, 0)), (True, (4, 2)), (True, (4, 4)), (True, (4, 6)), (True, (4, 8)), (True, (4, 10)), (True, (4, 18)), (True, (4, 20)), (True, (4, 22)), (True, (4, 24)), (True, (4, 26)), (True, (4, 12)), (True, (4, 14)), (True, (4, 16)), (True, (10, 0)), (True, (10, 2)), (True, (10, 4)), (True, (10, 6)), (True, (10, 8)), (True, (10, 10)), (True, (10, 18)), (True, (10, 20)), (True, (10, 22)), (True, (10, 24)), (True, (10, 26)), (True, (10, 12)), (True, (10, 14)), (True, (10, 16)), (True, (12, 0)), (True, (12, 2)), (True, (12, 4)), (True, (12, 6)), (True, (12, 8)), (True, (12, 10)), (True, (12, 18)), (True, (12, 20)), (True, (12, 22)), (True, (12, 24)), (True, (12, 26)), (True, (12, 12)), (True, (12, 14)), (True, (12, 16)), (True, (0, 0)), (True, (0, 2)), (True, (0, 4)), (True, (0, 6)), (True, (0, 8)), (True, (0, 10)), (True, (0, 18)), (True, (0, 20)), (True, (0, 22)), (True, (0, 24)), (True, (0, 26)), (True, (0, 12)), (True, (0, 14)), (True, (0, 16)), (True, (2, 0)), (True, (2, 2)), (True, (2, 4)), (True, (2, 6)), (True, (2, 8)), (True, (2, 10)), (True, (2, 18)), (True, (2, 20)), (True, (2, 22)), (True, (2, 24)), (True, (2, 26)), (True, (2, 12)), (True, (2, 14)), (True, (2, 16))]\n",
      "Cogroup: [(False, (<pyspark.resultiterable.ResultIterable object at 0x10676a950>, <pyspark.resultiterable.ResultIterable object at 0x10676a490>)), (True, (<pyspark.resultiterable.ResultIterable object at 0x10676a890>, <pyspark.resultiterable.ResultIterable object at 0x10676aa50>))]\n",
      "After groupByKey: [[(False, <pyspark.resultiterable.ResultIterable object at 0x10670cdd0>)], [(True, <pyspark.resultiterable.ResultIterable object at 0x10670c6d0>)], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Grouping and joining by key\n",
    "\n",
    "# There are various possible ways of joining 2 RDDs together by key:\n",
    "rddP2 = sc.parallelize(range(0, 28, 2)).map(lambda x: (x % 2 == 0, x))\n",
    "# inner join, or cross join in case of overlapping keys\n",
    "print(\"Join: {}\".format(rddP1.join(rddP2).collect()))\n",
    "# left/right outer join\n",
    "rddP1.leftOuterJoin(rddP2)\n",
    "rddP1.rightOuterJoin(rddP2)\n",
    "\n",
    "# for all keys in either rddP1 or rddP2, cogroup returns iterables of the values in either\n",
    "print(\"Cogroup: {}\".format(rddP1.cogroup(rddP2).collect()))\n",
    "# cogrouping together more than two RDDs by key can be done with groupWith\n",
    "rddP1.groupWith(rddP2, rddP2)\n",
    "\n",
    "# with groupByKey we create a new RDD that keeps the same keys on the same node, where possible\n",
    "print(\"After groupByKey: {}\".format(rddP1.groupByKey().glom().collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark can directly create RDDs from text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: addFile does not seem to work. Is it because we are running in standalone mode?\n",
    "# from pyspark import SparkFiles\n",
    "# sc.addFile(os.path.join(os.environ.get('SPARK_HOME'), 'LICENSE'))\n",
    "# rddT = sc.textFile(SparkFiles.get('LICENSE'))\n",
    "# print(rddT.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs support simple statistical actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(count: 14, mean: 6.5, stdev: 4.03112887415, max: 13.0, min: 0.0)\n",
      "14\n",
      "91\n",
      "6.5\n",
      "(4.0311288741492746, 4.1833001326703778)\n",
      "(16.25, 17.5)\n",
      "(0, 13)\n",
      "([0.0, 2.6, 5.2, 7.800000000000001, 10.4, 13], [3, 3, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "print(rdd.stats())\n",
    "print(rdd.count())\n",
    "print(rdd.sum())\n",
    "print(rdd.mean())\n",
    "print(rdd.stdev(), rdd.sampleStdev())\n",
    "print(rdd.variance(), rdd.sampleVariance())\n",
    "print(rdd.min(), rdd.max())\n",
    "print(rdd.histogram(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs support set transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24]\n",
      "[24, 16, 0, 8, 1, 9, 2, 10, 18, 3, 11, 4, 12, 20, 13, 5, 14, 6, 22, 7]\n",
      "[0, 8, 10, 2, 12, 4, 6]\n",
      "[9, 1, 11, 3, 5, 13, 7]\n",
      "[(0, 0), (0, 2), (0, 4), (1, 0), (1, 2), (1, 4), (2, 0), (2, 2), (2, 4), (0, 6), (0, 8), (0, 10), (1, 6), (1, 8), (1, 10), (2, 6), (2, 8), (2, 10), (0, 12), (0, 14), (0, 16), (1, 12), (1, 14), (1, 16), (2, 12), (2, 14), (2, 16), (0, 18), (0, 20), (0, 22), (1, 18), (1, 20), (1, 22), (2, 18), (2, 20), (2, 22), (0, 24), (1, 24), (2, 24), (3, 0), (3, 2), (3, 4), (4, 0), (4, 2), (4, 4), (5, 0), (5, 2), (5, 4), (3, 6), (3, 8), (3, 10), (4, 6), (4, 8), (4, 10), (5, 6), (5, 8), (5, 10), (3, 12), (3, 14), (3, 16), (4, 12), (4, 14), (4, 16), (5, 12), (5, 14), (5, 16), (3, 18), (3, 20), (3, 22), (4, 18), (4, 20), (4, 22), (5, 18), (5, 20), (5, 22), (3, 24), (4, 24), (5, 24), (6, 0), (6, 2), (6, 4), (7, 0), (7, 2), (7, 4), (8, 0), (8, 2), (8, 4), (6, 6), (6, 8), (6, 10), (7, 6), (7, 8), (7, 10), (8, 6), (8, 8), (8, 10), (6, 12), (6, 14), (6, 16), (7, 12), (7, 14), (7, 16), (8, 12), (8, 14), (8, 16), (6, 18), (6, 20), (6, 22), (7, 18), (7, 20), (7, 22), (8, 18), (8, 20), (8, 22), (6, 24), (7, 24), (8, 24), (9, 0), (9, 2), (9, 4), (10, 0), (10, 2), (10, 4), (11, 0), (11, 2), (11, 4), (12, 0), (12, 2), (12, 4), (13, 0), (13, 2), (13, 4), (9, 6), (9, 8), (9, 10), (10, 6), (10, 8), (10, 10), (11, 6), (11, 8), (11, 10), (12, 6), (12, 8), (12, 10), (13, 6), (13, 8), (13, 10), (9, 12), (9, 14), (9, 16), (10, 12), (10, 14), (10, 16), (11, 12), (11, 14), (11, 16), (12, 12), (12, 14), (12, 16), (13, 12), (13, 14), (13, 16), (9, 18), (9, 20), (9, 22), (10, 18), (10, 20), (10, 22), (11, 18), (11, 20), (11, 22), (9, 24), (10, 24), (11, 24), (12, 18), (12, 20), (12, 22), (13, 18), (13, 20), (13, 22), (12, 24), (13, 24)]\n"
     ]
    }
   ],
   "source": [
    "rddB = sc.parallelize(range(0, 26, 2))\n",
    "print(rdd.union(rddB).collect()) # or: rdd + rddB\n",
    "print(rdd.union(rddB).distinct().collect())\n",
    "print(rdd.intersection(rddB).collect())\n",
    "print(rdd.subtract(rddB).collect())\n",
    "print(rdd.cartesian(rddB).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark supports cluster-wide shared variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CA': 'California', 'NL': 'Netherlands'}\n",
      "91\n"
     ]
    }
   ],
   "source": [
    "# A broadcast variable is copied to each machine only once, in an efficient manner.\n",
    "# It is very suitable when each node uses the data in it, and especially so if the data is\n",
    "# large and would otherwise be sent across the network multiple times.\n",
    "broadcastVar = sc.broadcast({'CA': 'California', 'NL': 'Netherlands'})\n",
    "print(broadcastVar.value)\n",
    "\n",
    "# An accumulator is a shared variable that lives on the master, and to\n",
    "# which each task can add values. (Basically, it's a simple reducer.)\n",
    "accu = sc.accumulator(0)\n",
    "# 'foreach' just applies a function to each RDD element without returning anything\n",
    "rdd.foreach(lambda x: accu.add(x))\n",
    "print(accu.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark allows for customizable partitioning and parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First of all, a SparkContext instantiation can be used to set certain defaults\n",
    "sc.setLocalProperty('defaultMinPartitions', '4')\n",
    "sc.setLocalProperty('defaultParallelism', '4')\n",
    "\n",
    "# Also, aggregations such as 'reduceByKey' allow specifying the level of parallelism manually\n",
    "rddP1.reduceByKey(lambda x, y: x + y, 10)\n",
    "\n",
    "# Also, we can set any RDD to use a given number of partitions\n",
    "rddRepart = rdd.partitionBy(100)\n",
    "\n",
    "# Finally, there are some methods for manual re-partitioning of RDDs.\n",
    "# (Warning: these can be expensive, but in certain cases very useful.)\n",
    "\n",
    "# Efficient downscaling of partitions is done with 'coalesce'\n",
    "rddRepart = rdd.coalesce(2)\n",
    "# And full manual repartitioning\n",
    "rddRepart = rdd.repartition(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more complicated example of `partitionBy` is to use it for predefining how an RDD is partitioned, and to use this partitioning on other RDDs in exactly the same way. This way, we avoid shuffling the entire data set during, e.g., a `join` operation between these data sets. Depending on the application, this can translate into significant speedups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(False, 7), (False, 9), (False, 11), (False, 13), (False, 1), (False, 3), (False, 5)], [(True, 0), (True, 2), (True, 10), (True, 12), (True, 4), (True, 6), (True, 8)]]\n",
      "[[], [(True, 12), (True, 14), (True, 16), (True, 18), (True, 20), (True, 22), (True, 24), (True, 26), (True, 6), (True, 8), (True, 10), (True, 0), (True, 2), (True, 4)]]\n",
      "[[], [(True, (0, 18)), (True, (0, 20)), (True, (0, 22)), (True, (0, 24)), (True, (0, 26)), (True, (0, 12)), (True, (0, 14)), (True, (0, 16)), (True, (0, 6)), (True, (0, 8)), (True, (0, 10)), (True, (0, 0)), (True, (0, 2)), (True, (0, 4)), (True, (2, 18)), (True, (2, 20)), (True, (2, 22)), (True, (2, 24)), (True, (2, 26)), (True, (2, 12)), (True, (2, 14)), (True, (2, 16)), (True, (2, 6)), (True, (2, 8)), (True, (2, 10)), (True, (2, 0)), (True, (2, 2)), (True, (2, 4)), (True, (4, 18)), (True, (4, 20)), (True, (4, 22)), (True, (4, 24)), (True, (4, 26)), (True, (4, 12)), (True, (4, 14)), (True, (4, 16)), (True, (4, 6)), (True, (4, 8)), (True, (4, 10)), (True, (4, 0)), (True, (4, 2)), (True, (4, 4)), (True, (10, 18)), (True, (10, 20)), (True, (10, 22)), (True, (10, 24)), (True, (10, 26)), (True, (10, 12)), (True, (10, 14)), (True, (10, 16)), (True, (10, 6)), (True, (10, 8)), (True, (10, 10)), (True, (10, 0)), (True, (10, 2)), (True, (10, 4)), (True, (12, 18)), (True, (12, 20)), (True, (12, 22)), (True, (12, 24)), (True, (12, 26)), (True, (12, 12)), (True, (12, 14)), (True, (12, 16)), (True, (12, 6)), (True, (12, 8)), (True, (12, 10)), (True, (12, 0)), (True, (12, 2)), (True, (12, 4)), (True, (6, 18)), (True, (6, 20)), (True, (6, 22)), (True, (6, 24)), (True, (6, 26)), (True, (6, 12)), (True, (6, 14)), (True, (6, 16)), (True, (6, 6)), (True, (6, 8)), (True, (6, 10)), (True, (6, 0)), (True, (6, 2)), (True, (6, 4)), (True, (8, 18)), (True, (8, 20)), (True, (8, 22)), (True, (8, 24)), (True, (8, 26)), (True, (8, 12)), (True, (8, 14)), (True, (8, 16)), (True, (8, 6)), (True, (8, 8)), (True, (8, 10)), (True, (8, 0)), (True, (8, 2)), (True, (8, 4))]]\n"
     ]
    }
   ],
   "source": [
    "rddP1 = sc.parallelize(range(14)).map(lambda x: (x % 2 == 0, x)).partitionBy(2)\n",
    "rddP2 = sc.parallelize(range(0, 28, 2)).map(lambda x: (x % 2 == 0, x)).partitionBy(2)\n",
    "\n",
    "print(rddP1.glom().collect())\n",
    "print(rddP2.glom().collect())\n",
    "\n",
    "# Now this 'join' does not require a full shuffle of the data,\n",
    "# since the 'False' and 'True' keys are on the same node\n",
    "rddJ = rddP1.join(rddP2)\n",
    "# If we also want to keep 'rddJ' in the same partitioning, we have to specify it again\n",
    "rddJ = rddJ.partitionBy(2)\n",
    "print(rddJ.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pitfalls\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not caching intermediate results that are re-used later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not so great:\n",
      "1 loops, best of 1: 1.44 s per loop\n",
      "1 loops, best of 1: 1.49 s per loop\n",
      "1 loops, best of 1: 1.57 s per loop\n",
      "1 loops, best of 1: 1.18 s per loop\n",
      "Better:\n",
      "1 loops, best of 1: 2.35 s per loop\n",
      "1 loops, best of 1: 857 ms per loop\n",
      "1 loops, best of 1: 864 ms per loop\n",
      "1 loops, best of 1: 901 ms per loop\n"
     ]
    }
   ],
   "source": [
    "print(\"Not so great:\")\n",
    "rddBigTrans = rddBig.map(lambda x: (x ** 2 - 0.1) ** 0.5)\n",
    "for threshold in (0.2, 0.4, 0.6, 0.8):\n",
    "    %timeit -n 1 -r 1 rddBigTrans.filter(lambda x: x >= threshold).count()\n",
    "\n",
    "print(\"Better:\")\n",
    "rddBigTrans_c = rddBig.map(lambda x: (x ** 2 - 0.1) ** 0.5).cache()\n",
    "for threshold in (0.2, 0.4, 0.6, 0.8):\n",
    "    %timeit -n 1 -r 1 rddBigTrans_c.filter(lambda x: x >= threshold).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not considering when and how data is transfered through the cluster\n",
    "Keep in mind that Spark is a distributed computing framework, and that transferring data over the network within a cluster should be avoided (network bandwidth is ~100 times more expensive than memory bandwidth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# groupByKey triggers a shuffle so a lot of data is copied over the network\n",
    "sumPerKey = rddP1.groupByKey().mapValues(lambda x: sum(x)).collect()\n",
    "\n",
    "# Better: reduceByKey reduces locally before shuffling\n",
    "sumPerKey = rddP1.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not working with an appropriate number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Not enough partitions results in bad concurrency in the cluster.\n",
    "# It also puts pressure on the memory for certain operations.\n",
    "\n",
    "# On the other hand, suppose an RDD is distributed over 1000 partitions, but we work only on a small\n",
    "# subset of the data in the RDD, e.g.:\n",
    "rddF = rdd.filter(lambda x: x < 0.1).map(lambda x: x ** 2)\n",
    "\n",
    "# We are then effectively creating many empty tasks, and using coalesce or repartition\n",
    "# to create an RDD with less partitions would be beneficial:\n",
    "rddF = rdd.filter(lambda x: x < 3).coalesce(10).map(lambda x: x ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using map with high overhead per element, better to use mapPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For example, opening and closing a database connection takes time.\n",
    "def db_operation(x):\n",
    "    # Open connection to DB\n",
    "    # Do something with an element\n",
    "    # Close DB connection\n",
    "    pass\n",
    "\n",
    "# Especially so if you repeat it for every element:\n",
    "rdd.map(db_operation)\n",
    "\n",
    "# Better: do this at the level of partition rather than at the level of element.\n",
    "def vectorized_db_operation(x):\n",
    "    # Open connection to DB\n",
    "    # Do something with an array of elements\n",
    "    # Close DB connection\n",
    "    pass\n",
    "\n",
    "result = rdd.mapPartitions(vectorized_db_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending a lot of data along with a function call to each element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigData = np.random.random(int(1e6))\n",
    "\n",
    "def myFunc(x):\n",
    "    return x * np.random.choice(bigData)\n",
    "\n",
    "# this would send bigData along for each element of rdd\n",
    "rdd.map(myFunc)\n",
    "\n",
    "# Better: make the big data a read-only broadcast variable so that it\n",
    "# is efficiently copied across the network\n",
    "bigDataBC = sc.broadcast(bigData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code examples\n",
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split, ShuffleSplit\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn import pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "N = 10000   # number of data points\n",
    "D = 100     # number of dimensions\n",
    "\n",
    "X, y = make_regression(n_samples=N, n_features=D, n_informative=int(D*0.1),\n",
    "                       n_targets=1, bias=-6., noise=50., random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# partition the data into random subsamples\n",
    "samples = sc.parallelize(ShuffleSplit(y_train.size, n_iter=8))\n",
    "reg_model = pipeline.Pipeline([(\"scaler\", StandardScaler()), (\"ridge\", Ridge())])\n",
    "# train a model for each subsample and apply it to the test set\n",
    "mean_rsq = samples.map(\n",
    "    lambda (index, _): reg_model.fit(X[index], y[index]).score(X_test, y_test)).mean()\n",
    "print(mean_rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent using scikit-learn (from: https://gist.github.com/MLnick/4707012)\n",
    "Each partition is a mini-batch for the SGD, uses average weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm\n",
    "from sklearn.base import copy\n",
    "\n",
    "N = 10000   # Number of data points\n",
    "D = 10      # Numer of dimensions\n",
    "ITERATIONS = 5\n",
    "np.random.seed(seed=42)\n",
    "\n",
    "def generate_data(N):\n",
    "    return [[[1] if np.random.rand() < 0.5 else [0], np.random.randn(D)]\n",
    "            for _ in range(N)]\n",
    "\n",
    "def train(iterator, sgd):\n",
    "    for x in iterator:\n",
    "        sgd.partial_fit(x[1], x[0], classes=np.array([0, 1]))\n",
    "    yield sgd\n",
    "\n",
    "def merge(left, right):\n",
    "    new = copy.deepcopy(left)\n",
    "    new.coef_ += right.coef_\n",
    "    new.intercept_ += right.intercept_\n",
    "    return new\n",
    "\n",
    "def avg_model(sgd, slices):\n",
    "    sgd.coef_ /= slices\n",
    "    sgd.intercept_ /= slices\n",
    "    return sgd\n",
    "\n",
    "slices = 4\n",
    "data = generate_data(N)\n",
    "print(len(data))\n",
    "\n",
    "# init stochastic gradient descent\n",
    "sgd = lm.SGDClassifier(loss='log')\n",
    "# training\n",
    "for ii in range(ITERATIONS):\n",
    "    sgd = sc.parallelize(data, numSlices=slices) \\\n",
    "            .mapPartitions(lambda x: train(x, sgd)) \\\n",
    "            .reduce(lambda x, y: merge(x, y))\n",
    "    # averaging weight vector => iterative parameter mixtures\n",
    "    sgd = avg_model(sgd, slices)\n",
    "    print(\"Iteration %d:\" % (ii + 1))\n",
    "    print(\"Model: \")\n",
    "    print(sgd.coef_)\n",
    "    print(sgd.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark universe\n",
    "------------------\n",
    "\n",
    "Other interesting tools for Spark:\n",
    "\n",
    "- Spark SQL: http://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "- MLlib, Spark's machine learning library: http://spark.apache.org/docs/latest/mllib-guide.html\n",
    "- Spark Streaming, for streaming data applications: http://spark.apache.org/docs/latest/streaming-programming-guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "\n",
    "Spark documentation: https://spark.apache.org/docs/latest/index.html\n",
    "\n",
    "Spark programming guide: http://spark.apache.org/docs/latest/programming-guide.html\n",
    "\n",
    "PySpark documentation: https://spark.apache.org/docs/latest/api/python/index.html\n",
    "\n",
    "### Books\n",
    "\n",
    "Learning Spark: http://shop.oreilly.com/product/0636920028512.do\n",
    "\n",
    "(preview: https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/)\n",
    "\n",
    "### Talks (recommended to watch them in this order)\n",
    "\n",
    "Parallel programming with Spark: https://www.youtube.com/watch?v=7k4yDKBYOcw\n",
    "\n",
    "Advanced Spark features: https://www.youtube.com/watch?v=w0Tisli7zn4\n",
    "\n",
    "PySpark: Python API for Spark: https://www.youtube.com/watch?v=xc7Lc8RA8wE\n",
    "\n",
    "Understanding Spark performance: https://www.youtube.com/watch?v=NXp3oJHNM7E\n",
    "\n",
    "A deeper understanding of Spark's internals: https://www.youtube.com/watch?v=dmL0N3qfSc8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
